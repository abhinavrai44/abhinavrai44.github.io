<!DOCTYPE html>
<html lang="en">

<head>
  <!-- meta -->
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Reinforcement Learning</title>
  <meta content="" name="keywords">
  <meta content="" name="description">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Poppins:300,300i,400,400i,500,500i,600,600i,700,700i|Playfair+Display:400,400i,700,700i,900,900i" rel="stylesheet">

  <!-- Bootstrap CSS File -->
  <link href="../lib/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Libraries CSS Files -->
  <link href="../lib/ionicons/css/ionicons.min.css" rel="stylesheet">
  <link href="../lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet">
  <link href="../lib/magnific-popup/magnific-popup.css" rel="stylesheet">
  <link href="../lib/hover/hover.min.css" rel="stylesheet">

  <!-- Blog Stylesheet File -->
  <link href="../css/blog.css" rel="stylesheet">

  <!-- Main Stylesheet File -->
  <link href="../css/style.css" rel="stylesheet">

  <!-- Responsive css -->
  <link href="../css/responsive.css" rel="stylesheet">

  <!-- Favicon -->
  <link rel="shortcut icon" href="../images/favicon.png">

</head>

<body>

  <!-- start section navbar -->
  <nav id="main-nav-subpage" class="subpage-nav">
    <div class="row">
      <div class="container">

        <div class="responsive"><i data-icon="m" class="ion-navicon-round"></i></div>

        <ul class="nav-menu list-unstyled">
          <li><a href="../index.html" class="smoothScroll">Home</a></li>
          <li><a href="../index.html#about" class="smoothScroll">About</a></li>
          <li><a href="../index.html#portfolio" class="smoothScroll">Projects</a></li>
          <li><a href="../index.html#blog" class="smoothScroll">Published Work</a></li>
        </ul>
      </div>
    </div>
  </nav>
  <!-- End section navbar -->
  <!-- start section main content -->
  <div class="main-content paddsection">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-md-8 col-md-offset-2">
          <div class="row">
            <div class="container-main single-main">
              <div class="col-md-12">
                <div class="block-main mb-30">
                  <img src="../images/reinforcement.gif" class="img-responsive" alt="reviews2" style="height: 100%; width: 100%; object-fit: contain">
                  <div class="content-main single-post padDiv">
                    <div class="journal-txt">
                      <h4><a href="#">Reinforcement Learning</a></h4>
                    </div>
                    <div class="post-meta">
                      <ul class="list-unstyled mb-0">
                        <li class="author">by:<a href="#">Abhinav Rai and Ashar Hasan</a></li>
                        <!-- <li class="date">date:<a href="#">March 31, 2017</a></li> -->
                      </ul>
                    </div>
                    <p class="mb-30">In this project, I tried to model the motor control unit in a virtual environment. The virtual environment used is <a href="http://opensim.stanford.edu/"> OpenSim </a>- a biomechanical physics environment for musculoskeletal simulations. The musculoskeletal model used has 18 muscles which can be controlled and 9 degrees of freedom.</p>
                    <blockquote> Every action we take begins with our brain. We learn to coordinate our muscles using our brain and take actions such as standing, walking, jumping, etc for granted. This process by which humans or more broadly animals coordinate and activate their muscles using their brain is referred to as motor control. In this post we see that artificial neural networks can be used to mimic the way our brain controls the human body.
                    </blockquote>
                    <p class="mb-30">Q-learning cannot be applied straightforwardly for continuous action space. This is because finding greedy policy in continuous spaces requires optimization of action a<sub>t</sub> at every timestep, this optimization is not practical because of the large non-trivial action spaces. DDPG achieves this using the actor-critic approach. The actor critic function helps to represent the policy function independent of the value function. The actor takes as input the current state of the environment and gives an action as output. The critic gives a temporal difference error signal based upon the state and the resultant reward. The output obtained from the critic is used to update both the actor and critic.</p>
                    <p class="mb-30"> The actor and critic structures can be modelled as neural networks which try to choose an action from the continuous action space according to the current state to try and minimize the temporal-difference (TD) error signal each time step. However, when using neural networks for reinforcement learning, the algorithm assumes that the input samples are independent and identically distributed. But this assumption is wrong as the inputs obtained are sequential in nature. To tackle this DDPG uses a finite-sized buffer representing historical states called a replay buffer. All inputs to the actor are sampled from a minibatch from the replay buffer. Once the replay buffer is full, the oldest samples are removed. The input of the actor network is the current state, and the output is a single real value representing an action chosen from a continuous action space. The critic outputs the estimated Q-value of the current state and of the action chosen by the actor. The actor is updated using the deterministic policy gradient theorem. The critic is updated from the gradients obtained from the TD error signal.</p>

                    <p class="mb-30"> Some actions that were successfully taught to the model include standing on both legs, legs joined and in split position. Other actions included standing on one leg and crouching. Videos for these actions is shown above. </p>
                  </div>
                </div>
              </div>
              
          </div>
        </div>
      </div>
    </div>
  </div>
  <!--  </div> -->
  <!-- start section main content -->

  <!-- start section footer -->
  <div id="footer" class="text-center">
    <div class="container">
      <div class="socials-media text-center">

        <ul class="list-unstyled">
          <li><a href="https://www.linkedin.com/in/abhinav-rai-040331118/" target="_blank"><i class="ion-social-linkedin"></i></a></li>
          <li><a href="https://www.github.com/abhinavrai44" target="_blank"><i class="ion-social-github"></i></a></li>
        </ul>

      </div>

    </div>
  </div>
  <!-- End section footer -->

  <!-- JavaScript Libraries -->
  <script src="../lib/jquery/jquery.min.js"></script>
  <script src="../lib/jquery/jquery-migrate.min.js"></script>
  <script src="../lib/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="../lib/typed/typed.js"></script>
  <script src="../lib/owlcarousel/owl.carousel.min.js"></script>
  <script src="../lib/magnific-popup/magnific-popup.min.js"></script>
  <script src="../lib/isotope/isotope.pkgd.min.js"></script>

  <!-- Template Main Javascript File -->
  <script src="../js/main.js"></script>

</body>

</html>
